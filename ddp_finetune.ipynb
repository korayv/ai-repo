{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "d99245015e98b1db"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Finetuning Models in Parallel\n",
    "This is a jupyter notebook to train models from datasets in huggingface"
   ],
   "id": "c9ef0746e4fac2ab"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T15:20:05.618528Z",
     "start_time": "2025-03-26T15:19:47.873694Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install accelerate transformers datasets huggingface-hub",
   "id": "249bc1bef932a7c4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from accelerate import notebook_launcher\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def training_function():\n",
    "\"\"\"Training function is using autotokenizer to tokenize datasets and start finetune session\"\"\"\n",
    "\n",
    "     # 1. Import libraries inside the function\n",
    "    from transformers import (\n",
    "        AutoTokenizer,\n",
    "        AutoModelForCausalLM,\n",
    "        Trainer,\n",
    "        TrainingArguments,\n",
    "        DataCollatorForLanguageModeling\n",
    "    )\n",
    "    from datasets import load_dataset\n",
    "    from huggingface_hub import login\n",
    "\n",
    "    # 2. Login to Hugging Face\n",
    "\n",
    "    login(token=\"YOUR_HF_TOKEN_HERE\")\n",
    "\n",
    "     # 3. Load model and tokenizer\n",
    "    model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Ensure pad token is set\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "    # 4. Load dataset\n",
    "    dataset = load_dataset(\"Unisyn-corp/lead-vision\", split=\"train\")\n",
    "\n",
    "    # 5. Define tokenization\n",
    "    def tokenize_function(examples):\n",
    "        merged_texts = []\n",
    "        for msg_list in examples[\"messages\"]:\n",
    "            contents = [m[\"content\"] for m in msg_list]\n",
    "            merged_text = \" \".join(contents)\n",
    "            merged_texts.append(merged_text)\n",
    "\n",
    "        return tokenizer(\n",
    "            merged_texts,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=512\n",
    "        )\n",
    "\n",
    "    tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "    # 6. Create data collator\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False\n",
    "    )\n",
    "\n",
    "    # 7. Define DeepSpeed config\n",
    "    deepspeed_config = {\n",
    "        \"train_batch_size\": \"auto\",\n",
    "        \"gradient_accumulation_steps\": \"auto\",\n",
    "        \"zero_optimization\": {\n",
    "            \"stage\": 2,\n",
    "            \"contiguous_gradients\": True,\n",
    "            \"overlap_comm\": True,\n",
    "            \"reduce_scatter\": True,\n",
    "            \"reduce_bucket_size\": 5e8,\n",
    "            \"allgather_bucket_size\": 5e8\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # 8. Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"Lead-Vision-Finetuned-8B\",\n",
    "        per_device_train_batch_size=1,\n",
    "        bf16=True,  # Overridden by ds_config if needed\n",
    "        num_train_epochs=3,\n",
    "        logging_steps=50,\n",
    "        save_steps=200,\n",
    "        gradient_accumulation_steps=4,\n",
    "        deepspeed=deepspeed_config\n",
    "    )\n",
    "\n",
    "    # 9. Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset,\n",
    "        data_collator=data_collator\n",
    "    )\n",
    "\n",
    "    # 10. Resume training from checkpoint\n",
    "    trainer.train(\n",
    "        resume_from_checkpoint=\"Lead-Vision-Finetuned-8B/checkpoint-3000\"\n",
    "    )\n",
    "\n",
    "# 11. Launch training\n",
    "notebook_launcher(training_function, num_processes=8)\n"
   ],
   "id": "c26786d5355c52b3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
